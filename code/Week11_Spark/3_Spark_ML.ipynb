{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Preparation\n",
    "We check if we are in Google Colab.  If this is the case, install all necessary packages.\n",
    "\n",
    "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 3.3.2 with hadoop 3.3, Java 8 and Findspark to locate the spark in the system. The tools installation can be carried out inside the Jupyter Notebook of the Colab.\n",
    "Learn more from [A Must-Read Guide on How to Work with PySpark on Google Colab for Data Scientists!](https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "    !wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
    "    !tar xf spark-3.3.2-bin-hadoop3.tgz\n",
    "    !mv spark-3.3.2-bin-hadoop3 spark\n",
    "    !pip install -q findspark\n",
    "    import os\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "    os.environ[\"SPARK_HOME\"] = \"/content/spark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start a Local Cluster\n",
    "Use findspark.init() to start a local cluster.  If you plan to use remote cluster, skip the findspark.init() and change the cluster_url according."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_url = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(spark_url)\\\n",
    "        .appName('Spark ML')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL Data Preparation\n",
    "\n",
    "First, we read a csv file.  We can provide option such as delimiter and header.  We then rename the colume names to remove dot ('.') in the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'bank-additional-full.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"delimiter\", \";\").option(\"header\", True).csv(path)\n",
    "cols = [c.replace('.', '_') for c in df.columns]\n",
    "df = df.toDF(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age', 'duration', 'campaign', 'pdays', 'previous', 'nr_employed']\n",
    "for c in cols:\n",
    "    df = df.withColumn(c, col(c).cast('int'))\n",
    "\n",
    "cols = ['emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m']\n",
    "for c in cols:\n",
    "    df = df.withColumn(c, col(c).cast('double'))\n",
    "    \n",
    "df = df.withColumn('label', df.y.cast('boolean').cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data\n",
    "We split data into 80% training and 20% testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML Pipeline\n",
    "Pipeline is a serie of data transformation to transform data for training and inferring.  A column can contain categorical data or numerical data:\n",
    "- For categorical data, we have to convert to unique numeric value using **'StringIndexer'** and perform feature encoding with **'OneHotEncoder'**.\n",
    "- For numerical data, we do not have to do anything.\n",
    "\n",
    "Once we transform all features, we vectorize them into a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first setup a pipeline of all data transformation.\n",
    "- First, we transform all selected string columns\n",
    "  + use a Transformer, *StringIndexer*, to encode labels in the column to indices (stored in columnnameIndex)\n",
    "  + perform one hot encoder on the index to map the label index to a binary vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalAttributes = ['job', 'marital', 'education', 'default', \n",
    "                         'housing', 'loan', 'contact', \n",
    "                         'month', 'day_of_week', 'poutcome']\n",
    "for columnName in categoricalAttributes:\n",
    "    stringIndexer = StringIndexer(inputCol=columnName, outputCol=columnName+ \"Index\")\n",
    "    stages.append(stringIndexer)\n",
    "    oneHotEncoder = OneHotEncoder(inputCol=columnName+ \"Index\", outputCol=columnName + \"Vec\")\n",
    "    stages.append(oneHotEncoder)\n",
    "    \n",
    "categoricalCols = [s + \"Vec\" for s in categoricalAttributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericColumns = ['age', 'campaign', 'pdays', 'previous',\n",
    "           'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', \n",
    "                  'euribor3m', 'nr_employed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the feature columns into a single column in the dataframe\n",
    "\n",
    "allFeatureCols =  numericColumns + categoricalCols\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=allFeatureCols,\n",
    "    outputCol=\"features\")\n",
    "stages.append(vectorAssembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Pipeline\n",
    "We build 2 pipelines, feature transformation pipeline and ML pipeline.  This allows us to reuse the feature extraction pipeline with several ML algorithms.  **'fit'** method is called to create a model and we can use **'transform'** to actual transform or infer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline for feature extraction\n",
    "\n",
    "featurePipeline = Pipeline(stages=stages)\n",
    "featureOnlyModel = featurePipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we 'fit' a pipeline to the data, we have the model.\n",
    "As we put only 'Transfomer' in the pipeline, the model is for feature extraction only.\n",
    "\n",
    "We apply our feature extraction model with 'transform' operation to our training and testing data \n",
    "to create new DataFrames with 'features' column that can be used in the next pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingFeaturesDf = featureOnlyModel.transform(train_df)\n",
    "testFeaturesDf = featureOnlyModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainingFeaturesDF and testFeaturesDF are training and testing DataFrames with feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingFeaturesDf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(trainingFeaturesDf.columns) - set(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek into training features\n",
    "\n",
    "trainingFeaturesDf.select(\"features\", \"label\").rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline\n",
    "\n",
    "Spark ML supports several standard ML algorithm.  In this example, we demonstrate how to use logistic regression and decision tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAccuracy(results):\n",
    "    correct = results.filter(results['label'] == results['prediction']).count()\n",
    "    total = results.count()\n",
    "    return 1.0*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "Configure an machine learning pipeline, which consists of only one stage containing an estimator (classification) (Logistic regression in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "lrPipeline = Pipeline(stages=[lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the pipeline to create a model from the training data.  The logistic regression estimator looks for column 'features' and 'labels' to create an ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrPipelineModel = lrPipeline.fit(trainingFeaturesDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our trained model, we transform testFeaturesDf to predict the results.  The predicted results are stored in 'prediciton' column.  We then use our calculateAccuracy function to calculate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lrPipelineModel.transform(testFeaturesDf)\n",
    "print('LogisticRegression Model test accuracy = ', calculateAccuracy(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.select('label', 'prediction').rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTree Model\n",
    "\n",
    "Once again a ML pipeline is created with only an estimator in the pipeline.  We then fit the pipeline with the trainingFeaturesDf to train a model.  Then, we transform testFeaturesDf to predict the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol='label', featuresCol='features')\n",
    "dtPipeline = Pipeline(stages=[dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtPipelineModel = dtPipeline.fit(trainingFeaturesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dtPipelineModel.transform(testFeaturesDf)\n",
    "print('DecisionTree Model test accuracy = ', calculateAccuracy(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
